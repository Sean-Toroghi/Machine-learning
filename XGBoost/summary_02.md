# XGBoost - summary (v02) and key take aways 

References:
- Book (2024): XGBoost for Regression Predictive Modeling and Time Series Analysis
- Paper (2016): XGBoost: A Scalable Tree Boosting System [https://arxiv.org/abs/1603.02754]
- Website: XGBoost [link](https://xgboost.readthedocs.io/en/stable/index.html)

This summary is my takeaways and notes from reference book "XGBoost for Regression Predictive Modeling and Time Series Analysis" by Partha Pritam Deka and Joyce Weiner that was published in 2024, the XGboost website [link](https://xgboost.readthedocs.io/en/stable/index.html), and the original published paper in 2016 XGBoost: A Scalable Tree Boosting System [arxiv](https://arxiv.org/pdf/1603.02754).


---
# [XGBoost: A Scalable Tree Boosting System 2016 (2016) [arxiv](https://arxiv.org/pdf/1603.02754)

XGBoost was developed to address the following shorcommings of existing ensemble tree-boosting algorithms at the time in 2016: ability to handle large-scale data,  flexibility in defining customized optimization objectives and evaluation criteria, and support for parallel processing and distributed computing. 

Some of the enhancements added to the gradient-boosted trees to create XGBoost are:
- a sparsity-aware algorithm for handling missing values 
- a regularization term to control model complexity
- eficiency and scalability, achieved through a cache-aware block structure and parallel tree construction

## How boosting improves the performance of a classification and regression tree (CART) model 

In boosting, results from multiple iterations are combined to improve the overall classification or prediction. Boosting is an iterative method of combining multiple “weak learners” into a “strong learner.” A weak learner is a model that is just slightly better than guessing
---

# Gradient-boosted trees

Gradient-boosted trees are a type of classification and regression tree (CART) model, that learns via building a decision tree. The algorihtm employs the gradient descent algorithm to minimizing a loss function that compares the predicted value versus a target value. 


---

__Evaluation metrics__

XGboost provides a range of evaluation [metrics](https://xgboost.readthedocs.io/en/stable/parameter.html#learning-task-parameters). The default metrics are: rmse for regression, and logloss for classification. Based on objective, the model picks the appropriate metric, if not specified. For example for "rank:map" the default metric is _mean average precision_.

While there are over 20 metrics, here is the list of ones that are used more frequently:
- rmse: root mean square error
- rmsle: root mean square log error: Default metric of reg:squaredlogerror objective. This metric reduces errors generated by outliers in dataset. But because log function is employed, rmsle might output nan when prediction value is less than -1.
- mae: mean absolute error
- mape: mean absolute percentage error
- mphe: mean Pseudo Huber error. Default metric of reg:pseudohubererror objective.
- logloss: negative log-likelihood
- error: Binary classification error rate. It is calculated as #(wrong cases)/#(all cases). For the predictions, the evaluation will regard the instances with prediction value larger than 0.5 as positive instances, and the others as negative instances.
- error@t: a different than 0.5 binary classification threshold value could be specified by providing a numerical value through ‘t’.
- merror: Multiclass classification error rate. It is calculated as #(wrong cases)/#(all cases).
- mlogloss: Multiclass logloss.
- auc: Receiver Operating Characteristic Area under the Curve. Available for classification and learning-to-rank tasks.
  - When used with binary classification, the objective should be binary:logistic or similar functions that work on probability.
  - When used with multi-class classification, objective should be multi:softprob instead of multi:softmax, as the latter doesn’t output probability. Also the AUC is calculated by 1-vs-rest with reference class weighted by class prevalence.
  - When used with LTR task, the AUC is computed by comparing pairs of documents to count correctly sorted pairs. This corresponds to pairwise learning to rank. The implementation has some issues with average AUC around groups and distributed workers not being well-defined.
  - On a single machine the AUC calculation is exact. In a distributed environment the AUC is a weighted average over the AUC of training rows on each node. therefore, distributed AUC is an approximation sensitive to the distribution of data across workers. Use another metric in distributed environments if precision and reproducibility are important.
  - When input dataset contains only negative or positive samples, the output is NaN. The behavior is implementation defined, for instance, scikit-learn returns instead.
- aucpr: Area under the PR curve. Available for classification and learning-to-rank tasks.
- pre: Precision at _k_. Supports only learning to rank task.
- ndcg: Normalized Discounted Cumulative Gain
- map: Mean Average Precision
- ndcg@n, map@n, pre@n: can be assigned as an integer to cut off the top positions in the lists for evaluation.
- ndcg-, map-, ndcg@n-, map@n-: In XGBoost, the NDCG and MAP evaluate the score of a list without any positive samples as 1. By appending “-” to the evaluation metric name, we can ask XGBoost to evaluate these scores as  to be consistent under some conditions.
- poisson-nloglik: negative log-likelihood for Poisson regression
- gamma-nloglik: negative log-likelihood for gamma regression
- cox-nloglik: negative partial log-likelihood for Cox proportional hazards regression
- gamma-deviance: residual deviance for gamma regression
- tweedie-nloglik: negative log-likelihood for Tweedie regression (at a specified value of the tweedie_variance_power parameter)
- aft-nloglik: Negative log likelihood of Accelerated Failure Time model. See Survival Analysis with Accelerated Failure Time for details.
- interval-regression-accuracy: Fraction of data points whose predicted labels fall in the interval-censored labels. Only applicable for interval-censored data (Survival Analysis with Accelerated Failure Time for details.)

---
